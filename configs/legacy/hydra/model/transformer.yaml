type: classic_transformer
checkpoint_name: transformer.pt
params:
  # Architecture (capacity / speed tradeoff).
  # Main knobs: d_model, n_layer, n_head.
  d_model: 128
  n_layer: 3
  n_head: 4
  dropout: 0.1
  bias: true
  norm_eps: 0.000001

  # Runtime/optimization.
  # Main knobs: optimizer_weight_decay, grad_clip_norm, scheduler_*.
  use_torch_compile: true
  compile_mode: reduce-overhead
  compile_dynamic: false
  optimizer_fused: true
  optimizer_weight_decay: 0.01
  scheduler_warmup_ratio: 0.125
  scheduler_warmup_start_factor: 0.3
  scheduler_min_lr: 0.0000001
  grad_clip_norm: 0.5
  dataloader_num_workers: -1
  dataloader_prefetch_factor: 4
  dataloader_persistent_workers: true
  dataloader_pin_memory: true

  # Train-time masking (MIT/ORT objective inputs).
  # Use train_mask_mode=random for purely random corruption.
  train_mask_mode: block_feature
  train_missing_rate: 0.20
  train_block_min_len: 2
  train_block_max_len: 14
  train_block_missing_prob: 0.35
  train_feature_block_prob: 0.60
  train_block_no_overlap: true
