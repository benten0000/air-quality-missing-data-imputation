type: classic_transformer
checkpoint_name: transformer.pt
params:
  # Core model
  d_model: 128
  n_layer: 3
  n_head: 4
  dropout: 0.1
  bias: true
  norm_eps: 0.000001
  use_torch_compile: true
  compile_mode: reduce-overhead
  compile_dynamic: false
  optimizer_fused: true
  preload_data_to_gpu: true
  gpu_preload_max_mem_frac: 0.5
  dataloader_num_workers: 0
  dataloader_pin_memory: true

  # Train masking mode
  # - random: uses train_missing_rate
  # - block_feature: uses block params below
  train_mask_mode: block_feature

  # Random masking params (used when train_mask_mode = random)
  train_missing_rate: 0.20

  # Block-feature masking params (used when train_mask_mode = block_feature)
  # 1) sample block length in [train_block_min_len, train_block_max_len]
  # 2) with train_block_missing_prob decide if block is missing
  # 3) inside selected block, each feature is masked with train_feature_block_prob
  train_block_min_len: 2
  train_block_max_len: 14
  train_block_missing_prob: 0.35
  train_feature_block_prob: 0.60

  # If true, blocks are contiguous partition (no overlap stacking)
  # If false, sampled blocks may overlap
  train_block_no_overlap: true
