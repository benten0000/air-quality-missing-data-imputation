type: classic_transformer_moe
checkpoint_name: transformer_moe.pt
params:
  # Core model
  d_model: 128
  n_layer: 3
  n_head: 4
  dropout: 0.1
  bias: true
  norm_eps: 0.000001
  use_torch_compile: true
  compile_mode: reduce-overhead
  compile_dynamic: false
  optimizer_fused: true
  preload_data_to_gpu: true
  gpu_preload_max_mem_frac: 0.5
  dataloader_num_workers: -1
  dataloader_prefetch_factor: 2
  dataloader_persistent_workers: true
  dataloader_pin_memory: true

  # Train masking mode
  train_mask_mode: block_feature
  train_missing_rate: 0.20
  train_block_min_len: 2
  train_block_max_len: 14
  train_block_missing_prob: 0.35
  train_feature_block_prob: 0.60
  train_block_no_overlap: true

  # MoE feed-forward (DeepSeek-like top-k grouped routing)
  moe_enabled: true
  moe_num_experts: 16
  moe_top_k: 4
  moe_expert_width_mult: 1.0
  moe_n_shared_experts: 1
  moe_n_expert_groups: 4
  moe_n_limited_groups: 2
  moe_score_func: softmax
  moe_route_scale: 1.0
  moe_load_balance_weight: 0.01
  moe_use_shared_expert: true
